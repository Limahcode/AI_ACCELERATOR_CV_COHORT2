{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d7c9d7",
   "metadata": {},
   "source": [
    "### Introduction to Computer Vision and Image Processing Fundamentals\n",
    "\n",
    "## Content\n",
    "1. Defining Artificial Intelligence\n",
    "2. Why Computer Vision?\n",
    "3. Image Processing with Python and OpenCV\n",
    "## \n",
    "\n",
    "\n",
    "## What is AI?\n",
    "\n",
    "## Defining AI ‚Äî A Multi-Dimensional Approach\n",
    "\n",
    "AI can be defined along two primary dimensions: thinking vs. acting and human-like vs. ideal (rational) \n",
    "behavior. These give rise to four categories:\n",
    "\n",
    "## ‚óè Thinking Humanly: Mimicking human cognitive processes.\n",
    "## ‚óè Thinking Rationally: Emulating correct, logical reasoning.\n",
    "## ‚óè Acting Humanly: Imitating human behavior in observable actions.\n",
    "## ‚óè Acting Rationally: Making optimal decisions based on knowledge and expected outcomes.\n",
    "\n",
    "Each perspective offers unique insights and has shaped different subfields and methodologies within AI \n",
    "research.\n",
    "## Acting Humanly ‚Äî The Turing Test Approach\n",
    "Proposed by Alan Turing in 1950, the Turing Test evaluates whether a computer can mimic human \n",
    "responses so convincingly that a human interrogator cannot distinguish it from a person. To pass the \n",
    "Turing Test, a computer must demonstrate:\n",
    "\n",
    "## ‚óè Natural Language Processing: To understand and respond in human language.\n",
    "## ‚óè Knowledge Representation: To store and retrieve facts and beliefs.\n",
    "## ‚óè Automated Reasoning: To draw logical conclusions and answer questions.\n",
    "## ‚óè Machine Learning: To adapt and improve with experience.\n",
    " \n",
    "Although the Turing Test has symbolic significance, few researchers focus solely on passing it today.\n",
    "\n",
    "\n",
    "\n",
    "### Beyond the Turing Test ‚Äî The Total Turing Test\n",
    "\n",
    "## The Total Turing Test extends the original concept by including perceptual and physical tasks:\n",
    "## ‚óè Computer Vision: To visually interpret the world.\n",
    "## ‚óè Robotics: To physically interact with and navigate environments.\n",
    " \n",
    "## ‚óè Turing‚Äôs test remains influential not because it defines AI‚Äôs endpoint, but because it highlights essential competencies required for general intelligence. Modern researchers, however, prioritize understanding core principles over mimicking humans perfectly.\n",
    "\n",
    "### Thinking Humanly ‚Äî The Cognitive Modeling Approach\n",
    "To say a program thinks like a human requires understanding human cognition itself. Cognitive \n",
    "modeling uses:\n",
    "\n",
    "## ‚óè Introspection: Examining one‚Äôs own thought processes.\n",
    "## ‚óè Psychological Experiments: Observing human behavior in controlled settings.\n",
    "## ‚óè Neuroimaging: Studying brain activity.\n",
    " \n",
    "By developing computational models that simulate human thought patterns, researchers can test \n",
    "theories about cognition. The interdisciplinary field of cognitive science merges AI and \n",
    "psychology to create testable, mechanistic explanations of mental processes.\n",
    "\n",
    "## Thinking Rationally ‚Äî The ‚ÄúLaws of Thought‚Äù Approach\n",
    "Dating back to Aristotle, this approach centers on codifying perfect reasoning:\n",
    "## ‚óè Syllogisms: Structured arguments that yield true conclusions from true premises.\n",
    "## ‚óè Logic: A formal language for expressing and manipulating knowledge.\n",
    " \n",
    "## ‚óè 19th-century logicians laid the groundwork for programs that could solve problems expressed in logical form. However, translating human knowledge into formal logic is challenging, and reasoning systems often struggle with computational complexity. These issues gave rise to more flexible AI paradigms that incorporate uncertainty and practical constraints.\n",
    "\n",
    "\n",
    "### Acting Rationally ‚Äî The Rational Agent Approach\n",
    "The rational agent model defines an AI as an agent that perceives its environment and acts to \n",
    "maximize expected outcomes. It encompasses:\n",
    "\n",
    "## ‚óè Autonomy: Operating independently.\n",
    "## ‚óè Adaptability: Learning and adjusting to new circumstances.\n",
    "## ‚óè Goal-orientation: Pursuing objectives intelligently.\n",
    " \n",
    "## ‚óè Rationality includes logical inference but also reflexes and heuristics that bypass explicit reasoning when necessary. This framework is more general and scientifically tractable than human-centered approaches, making it the dominant paradigm in AI today.\n",
    "\n",
    "### Rationality and Real-World Constraints\n",
    "\n",
    "## ‚óè While rationality is a compelling ideal, real-world environments are too complex for perfect rationality.\n",
    "\n",
    "## ‚óè Computational limitations and incomplete information force systems to adopt bounded rationality‚Äîacting effectively under constraints. \n",
    "## ‚óè Modern AI uses the rational agent as a foundation while acknowledging that trade-offs and approximations are inevitable. \n",
    "## ‚óè The journey toward general intelligence involves embracing this complexity while designing agents that make good decisions in practice.\n",
    "\n",
    "\n",
    "### Building a Rational Agent\n",
    "\n",
    "### What Makes an Agent?\n",
    "\n",
    "Core Components and Characteristics\n",
    "‚óè Agent: \n",
    "\n",
    "Anything that perceives its \n",
    "environment through sensors and acts \n",
    "through actuators\n",
    "\n",
    "‚óè Environment: The world in which the \n",
    "agent operates\n",
    "\n",
    "‚óè Sensors: Mechanisms for perceiving the \n",
    "environment\n",
    "\n",
    "‚óè Actuators: Mechanisms for acting upon \n",
    "the environment\n",
    "\n",
    "‚óè Coupling: The interaction between agent \n",
    "and environment determines behavior\n",
    "\n",
    "### Agent Examples Across Domains\n",
    "\n",
    "## From Humans to Software\n",
    "\n",
    "## Human Agent:\n",
    "‚óè Sensors: Eyes, ears, other organs\n",
    "\n",
    "‚óè Actuators: Hands, legs, vocal tract\n",
    "\n",
    "## Robotic Agent:\n",
    "‚óè Sensors: Cameras, infrared range finders\n",
    "‚óè Actuators: Various motors\n",
    "## Software Agent:\n",
    "‚óè Sensors: Keystrokes, file contents, network packets.\n",
    "‚óè Actuators: Screen display, file writing, network transmission\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9063a23",
   "metadata": {},
   "source": [
    "### Percepts and Percept Sequences\n",
    "## How Agents Process Information\n",
    "\n",
    "‚óè Percept: Agent's perceptual input at any given instant\n",
    "\n",
    "‚óè Percept Sequence: Complete history of everything the agent has ever \n",
    "perceived\n",
    "\n",
    "‚óè Agent's action choice can depend on entire percept sequence observed \n",
    "to date\n",
    "\n",
    "‚óè Actions cannot depend on unperceived information\n",
    "\n",
    "‚óè This forms the foundation for agent behavior analysis\n",
    "\n",
    "### Why Computer Vision?\n",
    "\n",
    "### The Perception Problem\n",
    "\n",
    "Rational Agents Need Rich Environmental Input\n",
    "\n",
    "## Remember our rational agent framework:\n",
    "‚óè Percepts: Agent's perceptual input at any given instant\n",
    "\n",
    "‚óè Sensors: Mechanisms for perceiving the environment\n",
    "\n",
    "‚óè Actions: Cannot depend on unperceived information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74efb8f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Challenge: \n",
    "Most real-world environments are primarily visual. Without computer vision, our agents are essentially blind to the richest source of environmental information.\n",
    "\n",
    "## Essential for General Intelligence\n",
    "## The Total Turing Test Connection\n",
    "Original Turing Test Requirements:\n",
    "\n",
    "‚óè Natural Language Processing\n",
    "\n",
    "‚óè Knowledge Representation\n",
    "\n",
    "‚óè Automated Reasoning\n",
    "\n",
    "‚óè Machine Learning\n",
    "## Total Turing Test Additions:\n",
    "\n",
    "‚óè Computer Vision ‚Üê Critical!\n",
    "\n",
    "‚óè Robotics\n",
    "\n",
    "Computer vision was recognized as essential for true AI because it enables machines to \"visually \n",
    "interpret the world\" - a fundamental requirement for general intelligence.\n",
    "\n",
    "### Expanding Agent Capabilities\n",
    "    ## From Limited to Rich Environmental Understanding\n",
    "## Without Computer Vision:\n",
    "\n",
    "‚óè Agents limited to text, audio, or simple sensor inputs\n",
    "\n",
    "‚óè Cannot understand spatial relationships\n",
    "\n",
    "‚óè Miss critical environmental context\n",
    "\n",
    "‚óè Restricted to narrow, specialized domains\n",
    "\n",
    "## With Computer Vision:\n",
    "\n",
    "‚óè Access to rich visual information (colors, shapes, movement, depth)\n",
    "\n",
    "‚óè Understanding of spatial relationships and object interactions\n",
    "\n",
    "‚óè Real-time environmental awareness\n",
    "\n",
    "‚óè Foundation for autonomous navigation, manipulation, and decision-making\n",
    "\n",
    "### Where Computer Vision Enables Rational Agents\n",
    "üöó Autonomous Systems:\n",
    "\n",
    "‚óè Self-driving cars\n",
    "\n",
    "‚óè Delivery drones\n",
    "\n",
    "‚óè Manufacturing robots\n",
    "\n",
    "üè• Healthcare & Safety:\n",
    "\n",
    "‚óè Medical image analysis\n",
    "\n",
    "‚óè Security surveillance\n",
    "\n",
    "‚óè Quality control\n",
    "\n",
    "üì± Human-Computer Interaction:\n",
    "\n",
    "‚óè Gesture recognition\n",
    "\n",
    "‚óè Augmented reality\n",
    "\n",
    "‚óè Facial recognition\n",
    "\n",
    "üî¨ Scientific Discovery:\n",
    "\n",
    "‚óè Astronomical imaging\n",
    "\n",
    "‚óè Microscopy analysis\n",
    "\n",
    "‚óè Environmental monitoring\n",
    "\n",
    "## Bottom Line: \n",
    "Computer vision transforms AI from narrow, text-based systems into agents that can perceive, understand, and act in our visual world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d054c5d",
   "metadata": {},
   "source": [
    "### Image Processing with Python & OpenCV\n",
    "## Background: OpenCV and NumPy\n",
    "\n",
    "OpenCV (Open Source Computer Vision Library): https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html\n",
    "\n",
    "‚óè A powerful open-source library for real-time computer vision, image processing, and machine learning.\n",
    "\n",
    "‚óè Written in C/C++ but has full Python bindings (cv2).\n",
    "\n",
    "‚óè Supports:\n",
    "\n",
    "‚óã Reading/writing images and videos\n",
    "\n",
    "‚óã Image transformations (resizing, filtering, rotation)\n",
    "\n",
    "‚óã Feature detection, object tracking, face recognition, etc.\n",
    "\n",
    "‚óã Integration with deep learning models\n",
    "\n",
    "Originally developed by Intel in 1999, now maintained by OpenCV.org.\n",
    "\n",
    "### NumPy (Numerical Python) \n",
    "\n",
    "‚óè A core Python library for efficient numerical and matrix operations. https://numpy.org/doc/\n",
    "\n",
    "‚óè Provides the ndarray object ‚Äî a fast, flexible multi-dimensional array structure.\n",
    "\n",
    "‚óè Powers nearly all scientific computing in Python (used in pandas, scikit-learn, etc.).\n",
    "\n",
    "### How OpenCV and NumPy Are Related\n",
    "\n",
    "‚óè OpenCV uses NumPy arrays to represent images.\n",
    "\n",
    "‚óã When you load an image using cv2.imread(), you get a NumPy array of shape (H, W, 3) for color images or (H, \n",
    "W) for grayscale.\n",
    "\n",
    "‚óè This means you can:\n",
    "\n",
    "‚óã Slice, index, and modify images using standard NumPy syntax.\n",
    "\n",
    "‚óã Apply mathematical operations (e.g., brightness scaling, masking, filtering).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dd386",
   "metadata": {},
   "source": [
    "### Practical Session\n",
    "\n",
    "## Github Repo: \n",
    "https://github.com/Limahcode/AI_ACCELERATOR_CV_COHORT2.git\n",
    "\n",
    "## Use the notebook in Google Colab:\n",
    "https://colab.research.google.com/drive/1OvO5BV6i7rqNhKemGyONJSeOvatNTv\n",
    "Hc?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7375031",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
